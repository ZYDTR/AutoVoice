import os
import time
import torch
import re
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹ IDï¼Œä¼šè‡ªåŠ¨ä» ModelScope ä¸‹è½½
MODEL_ID = "iic/SenseVoiceSmall"

# è®¾å¤‡é€‰æ‹©ç­–ç•¥
DEVICE = "cpu" 
THREADS = 4 

# è¯´è¯äººè¯†åˆ«é…ç½®
ENABLE_SPEAKER_DIARIZATION = True  # è®¾ç½®ä¸º True å¯ç”¨è¯´è¯äººåŒºåˆ†
SPK_MODEL = "cam++"  # è¯´è¯äººè¯†åˆ«æ¨¡å‹

# ===========================================

def remove_emoji(text):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ emojiï¼Œä¿ç•™æ ‡ç‚¹ç¬¦å·å’ŒåŸºæœ¬å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰"""
    # æ³¨æ„ï¼šç§»é™¤äº† \U000024C2-\U0001F251 èŒƒå›´ï¼Œå› ä¸ºå®ƒåŒ…å«äº†ä¸­æ–‡å­—ç¬¦èŒƒå›´
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # æ——å¸œ
        "\U00002702-\U000027B0"  # å…¶ä»–ç¬¦å·
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001FA00-\U0001FA6F"  # æ‰©å±•ç¬¦å·
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•ç¬¦å·
        "\U00002600-\U000026FF"  # æ‚é¡¹ç¬¦å·
        "\U00002700-\U000027BF"  # è£…é¥°ç¬¦å·
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub('', text).strip()

def setup_model():
    print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ (Device: {DEVICE})...")
    if ENABLE_SPEAKER_DIARIZATION:
        print(f"ğŸ“¢ å·²å¯ç”¨è¯´è¯äººåŒºåˆ†åŠŸèƒ½ (æ¨¡å‹: {SPK_MODEL})")
    
    start_time = time.time()
    
    # AutoModel åˆå§‹åŒ–
    model_kwargs = {
        "model": MODEL_ID,
        "trust_remote_code": True,
        "vad_model": "fsmn-vad",   # å¼€å¯ VAD
        "vad_kwargs": {"max_single_segment_time": 30000}, # å¼ºåˆ¶æ¯æ®µæœ€é•¿ 30s
        "device": DEVICE,
        "ncpu": THREADS,
        "punc_model": "ct-punc"  # æ˜¾å¼æŒ‡å®šæ ‡ç‚¹ç¬¦å·æ¨¡å‹ï¼Œé¿å… punc_res é”™è¯¯
    }
    
    # å¦‚æœå¯ç”¨è¯´è¯äººè¯†åˆ«ï¼Œæ·»åŠ  spk_model
    if ENABLE_SPEAKER_DIARIZATION:
        model_kwargs["spk_model"] = SPK_MODEL
        # spk_kwargs å¯ä»¥ç”¨äºé…ç½®è¯´è¯äººè¯†åˆ«å‚æ•°
        # ä¾‹å¦‚ï¼šspk_kwargs={"threshold": 0.5}  # è¯´è¯äººç›¸ä¼¼åº¦é˜ˆå€¼
        print(f"   â””â”€ å·²è‡ªåŠ¨åŠ è½½æ ‡ç‚¹ç¬¦å·æ¨¡å‹ï¼ˆè¯´è¯äººè¯†åˆ«éœ€è¦ï¼‰")
    
    model = AutoModel(**model_kwargs)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")
    return model

def process_audio(model, audio_file):
    if not os.path.exists(audio_file):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ {audio_file} ä¸å­˜åœ¨")
        return None

    print(f"ğŸ™ï¸ æ­£åœ¨å¤„ç†éŸ³é¢‘: {os.path.basename(audio_file)}...")
    start_time = time.time()

    # æ‰§è¡Œæ¨ç† Pipeline
    res = model.generate(
        input=audio_file,
        cache={},
        language="auto",  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ (zh, en, yue, ja, ko)
        use_itn=True,     # å¼€å¯é€†æ–‡æœ¬æ ‡å‡†åŒ– (ä¾‹å¦‚: "ä¸€ç™¾" -> "100")
        batch_size_s=60,  # åŠ¨æ€æ‰¹å¤„ç†ï¼šæ¯æ‰¹å¤„ç† 60ç§’ çš„éŸ³é¢‘æ•°æ®
        merge_vad=True,   # å°†åˆ‡ç¢çš„ VAD ç‰‡æ®µåˆå¹¶æˆæ•´å¥
    )
    
    inference_time = time.time() - start_time
    
    # ç»“æœè§£æ
    if res:
        # res å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼Œéœ€è¦åˆ†åˆ«å¤„ç†
        if isinstance(res, list):
            if len(res) > 0:
                result_item = res[0]
                if isinstance(result_item, dict):
                    text = rich_transcription_postprocess(result_item.get("text", ""))
                    # ç§»é™¤ emoji
                    text = remove_emoji(text)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯´è¯äººä¿¡æ¯
                    speaker_info = result_item.get("spk", None)
                    if speaker_info:
                        print(f"ğŸ“¢ æ£€æµ‹åˆ°è¯´è¯äººä¿¡æ¯: {speaker_info}")
                    return {"text": text, "speaker": speaker_info, "raw": result_item}
                else:
                    text = rich_transcription_postprocess(result_item if result_item else "")
                    # ç§»é™¤ emoji
                    text = remove_emoji(text)
                    return {"text": text, "speaker": None, "raw": result_item}
            else:
                return None
        elif isinstance(res, dict):
            text = rich_transcription_postprocess(res.get("text", ""))
            # ç§»é™¤ emoji
            text = remove_emoji(text)
            speaker_info = res.get("spk", None)
            if speaker_info:
                print(f"ğŸ“¢ æ£€æµ‹åˆ°è¯´è¯äººä¿¡æ¯: {speaker_info}")
            return {"text": text, "speaker": speaker_info, "raw": res}
        else:
            text = str(res)
            # ç§»é™¤ emoji
            text = remove_emoji(text)
            return {"text": text, "speaker": None, "raw": res}
    else:
        return None

def format_result_with_speaker(result, audio_file):
    """æ ¼å¼åŒ–å¸¦è¯´è¯äººä¿¡æ¯çš„ç»“æœ"""
    if not result:
        return "æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³"
    
    text = result.get("text", "")
    speaker_info = result.get("speaker", None)
    raw_data = result.get("raw", {})
    
    output_lines = []
    output_lines.append(f"éŸ³é¢‘æ–‡ä»¶: {os.path.basename(audio_file)}\n")
    output_lines.append("="*60 + "\n")
    
    # å¦‚æœæœ‰è¯´è¯äººä¿¡æ¯ï¼Œæ ¼å¼åŒ–è¾“å‡º
    if speaker_info:
        output_lines.append("ğŸ“¢ è¯´è¯äººåŒºåˆ†ç»“æœ:\n")
        output_lines.append("-"*60 + "\n")
        
        # speaker_info å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼Œéœ€è¦æ ¹æ®å®é™…æ ¼å¼å¤„ç†
        if isinstance(speaker_info, list):
            for idx, spk in enumerate(speaker_info):
                if isinstance(spk, dict):
                    spk_id = spk.get("spk_id", f"Speaker_{idx}")
                    timestamp = spk.get("timestamp", "")
                    output_lines.append(f"è¯´è¯äºº {spk_id}: {timestamp}\n")
                else:
                    output_lines.append(f"è¯´è¯äºº {idx}: {spk}\n")
        elif isinstance(speaker_info, dict):
            for spk_id, info in speaker_info.items():
                output_lines.append(f"è¯´è¯äºº {spk_id}: {info}\n")
        else:
            output_lines.append(f"è¯´è¯äººä¿¡æ¯: {speaker_info}\n")
        
        output_lines.append("\n")
    
    # è½¬å½•æ–‡æœ¬
    output_lines.append("è¯†åˆ«ç»“æœ:\n")
    output_lines.append("-"*60 + "\n")
    output_lines.append(text + "\n")
    
    # å¦‚æœæœ‰åŸå§‹æ•°æ®ä¸­çš„æ—¶é—´æˆ³ä¿¡æ¯ï¼Œä¹Ÿè¾“å‡º
    if isinstance(raw_data, dict):
        timestamp = raw_data.get("timestamp", None)
        if timestamp:
            output_lines.append("\næ—¶é—´æˆ³ä¿¡æ¯:\n")
            output_lines.append(f"{timestamp}\n")
    
    return "".join(output_lines)

if __name__ == "__main__":
    # 1. åŠ è½½æ¨¡å‹
    model = setup_model()
    
    # 2. å¤„ç† recordings ç›®å½•ä¸‹çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    recordings_dir = "/Users/zhengyidi/AutoVoice/recordings"
    
    if not os.path.exists(recordings_dir):
        print(f"âŒ é”™è¯¯: ç›®å½• {recordings_dir} ä¸å­˜åœ¨")
    else:
        # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        audio_files = [f for f in os.listdir(recordings_dir) 
                      if f.endswith(('.webm', '.mp3', '.wav', '.m4a', '.flac'))]
        
        if not audio_files:
            print(f"âš ï¸ åœ¨ {recordings_dir} ä¸­æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
        else:
            print(f"\nğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\n")
            
            for idx, audio_file in enumerate(sorted(audio_files), 1):
                audio_path = os.path.join(recordings_dir, audio_file)
                print(f"\n{'='*60}")
                print(f"æ–‡ä»¶ {idx}/{len(audio_files)}: {audio_file}")
                print(f"{'='*60}")
                
                result = process_audio(model, audio_path)
                
                if result:
                    print("\n" + "="*20 + " è¯†åˆ«ç»“æœ " + "="*20)
                    formatted_result = format_result_with_speaker(result, audio_path)
                    print(formatted_result)
                    print("="*50)
                    
                    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                    output_file = os.path.join(recordings_dir, f"{os.path.splitext(audio_file)[0]}_transcription.txt")
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(formatted_result)
                    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}\n")
            
            print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
        
        # è¯´æ˜ï¼š
        # 1. è¯´è¯äººåŒºåˆ†åŠŸèƒ½éœ€è¦å¯ç”¨ spk_model="cam++"
        # 2. è¾“å‡ºç»“æœä¸­ä¼šåŒ…å«è¯´è¯äººIDå’Œæ—¶é—´æˆ³ä¿¡æ¯
        # 3. å¦‚æœéŸ³é¢‘ä¸­åªæœ‰ä¸€ä¸ªè¯´è¯äººï¼Œå¯èƒ½ä¸ä¼šæ˜¾ç¤ºè¯´è¯äººåŒºåˆ†ä¿¡æ¯
        # 4. å¤šè¯´è¯äººåœºæ™¯ä¸‹ï¼Œæ¯ä¸ªè¯´è¯äººçš„è¯­éŸ³ä¼šè¢«æ ‡è®°å¹¶åŒºåˆ†


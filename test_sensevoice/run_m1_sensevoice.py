import os
import time
import torch
import re
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

# ================= é…ç½®åŒºåŸŸ =================
# å¤„ç†æ¨¡å¼é€‰æ‹©
PROCESSING_MODE = "direct"  # "direct" æˆ– "cascaded"
# - "direct": ç›´æ¥ä½¿ç”¨å•ä¸€æ¨¡å‹ï¼ˆSenseVoice æˆ– Paraformerï¼‰
# - "cascaded": çº§è”æ¨¡å¼ï¼ˆParaformer åš diarization + SenseVoice è¯†åˆ«æ–‡æœ¬ï¼‰

# æ¨¡å‹é€‰æ‹©ï¼šæ”¯æŒ SenseVoice å’Œ Paraformerï¼ˆä»…åœ¨ direct æ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
# SenseVoice: ä¸æ”¯æŒ speaker diarization
# Paraformer: æ”¯æŒ speaker diarization
USE_MODEL = "sensevoice"  # "sensevoice" æˆ– "paraformer"

SENSEVOICE_MODEL = "iic/SenseVoiceSmall"
PARAFORMER_MODEL = "paraformer-zh"

# è¯´è¯äººè¯†åˆ«é…ç½®ï¼ˆä»… Paraformer æ”¯æŒï¼Œä»…åœ¨ direct æ¨¡å¼ä¸‹æœ‰æ•ˆï¼‰
ENABLE_SPEAKER_DIARIZATION = False  # è®¾ç½®ä¸º True å¯ç”¨è¯´è¯äººåŒºåˆ†ï¼ˆéœ€è¦ USE_MODEL="paraformer"ï¼‰
SPK_MODEL = "cam++"  # è¯´è¯äººè¯†åˆ«æ¨¡å‹

# è®¾å¤‡é€‰æ‹©ç­–ç•¥
# M1 Pro å»ºè®®ï¼š
# å¯¹äº < 1åˆ†é’Ÿçš„çŸ­éŸ³é¢‘ï¼Œæˆ–è€…è¿½æ±‚ç¨³å®šæ€§ï¼Œä½¿ç”¨ "cpu" æ˜¯æœ€ä½³é€‰æ‹©ï¼Œé€Ÿåº¦æå¿«ä¸”æ— å…¼å®¹æ€§é—®é¢˜ã€‚
# å¯¹äº > 10åˆ†é’Ÿçš„é•¿éŸ³é¢‘æ‰¹å¤„ç†ï¼Œå¯ä»¥å°è¯• "mps" (Metal Performance Shaders)ã€‚
# å¦‚æœé‡åˆ°æŠ¥é”™ï¼Œè¯·å›é€€åˆ° "cpu"ã€‚
DEVICE = "cpu" 
# DEVICE = "mps" # å–æ¶ˆæ³¨é‡Šä»¥å°è¯• GPU åŠ é€Ÿ

# çº¿ç¨‹æ•°è®¾ç½® (ä»…å¯¹ CPU æ¨¡å¼æœ‰æ•ˆ)
# M1 Pro æœ‰ 8 æˆ– 10 ä¸ªæ ¸å¿ƒï¼Œè®¾ç½®ä¸º 4-8 ä¹‹é—´é€šå¸¸æ•ˆç‡æœ€é«˜
THREADS = 4 

# ===========================================

def setup_model():
    # æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹
    if USE_MODEL == "paraformer":
        model_id = PARAFORMER_MODEL
        model_name = "Paraformer"
    else:
        model_id = SENSEVOICE_MODEL
        model_name = "SenseVoice"
    
    print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {model_name} (Device: {DEVICE})...")
    
    if ENABLE_SPEAKER_DIARIZATION:
        if USE_MODEL != "paraformer":
            print("âš ï¸ è­¦å‘Š: SenseVoice æ¨¡å‹ä¸æ”¯æŒ speaker diarizationï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
            enable_spk = False
        else:
            enable_spk = True
            print(f"ğŸ“¢ å·²å¯ç”¨è¯´è¯äººåŒºåˆ†åŠŸèƒ½ (æ¨¡å‹: {SPK_MODEL})")
    else:
        enable_spk = False
    
    start_time = time.time()
    
    # AutoModel åˆå§‹åŒ–
    model_kwargs = {
        "model": model_id,
        "trust_remote_code": True,
        "vad_model": "fsmn-vad",   # å¼€å¯ VAD
        "vad_kwargs": {"max_single_segment_time": 30000}, # å¼ºåˆ¶æ¯æ®µæœ€é•¿ 30s
        "device": DEVICE,
        "ncpu": THREADS,
        "punc_model": "ct-punc"  # æ˜¾å¼æŒ‡å®šæ ‡ç‚¹ç¬¦å·æ¨¡å‹ï¼Œé¿å… punc_res é”™è¯¯
    }
    
    # å¦‚æœå¯ç”¨è¯´è¯äººè¯†åˆ«ï¼Œæ·»åŠ  spk_modelï¼ˆä»… Paraformerï¼‰
    if enable_spk:
        model_kwargs["spk_model"] = SPK_MODEL
        print(f"   â””â”€ å·²è‡ªåŠ¨åŠ è½½æ ‡ç‚¹ç¬¦å·æ¨¡å‹ï¼ˆè¯´è¯äººè¯†åˆ«éœ€è¦ï¼‰")
        print(f"   â„¹ï¸ è¾“å‡ºæ—¶å°†è¿‡æ»¤æ‰ timestampï¼Œåªæ˜¾ç¤ºè¯´è¯äºº ID å’Œæ–‡æœ¬")
    
    model = AutoModel(**model_kwargs)
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")
    return model

def remove_emoji(text):
    """ç§»é™¤æ–‡æœ¬ä¸­çš„ emojiï¼Œä¿ç•™æ ‡ç‚¹ç¬¦å·å’ŒåŸºæœ¬å­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰"""
    # æ³¨æ„ï¼šç§»é™¤äº† \U000024C2-\U0001F251 èŒƒå›´ï¼Œå› ä¸ºå®ƒåŒ…å«äº†ä¸­æ–‡å­—ç¬¦èŒƒå›´
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # æ——å¸œ
        "\U00002702-\U000027B0"  # å…¶ä»–ç¬¦å·
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œè±¡å½¢æ–‡å­—
        "\U0001FA00-\U0001FA6F"  # æ‰©å±•ç¬¦å·
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•ç¬¦å·
        "\U00002600-\U000026FF"  # æ‚é¡¹ç¬¦å·
        "\U00002700-\U000027BF"  # è£…é¥°ç¬¦å·
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub('', text).strip()

def process_audio(model, audio_file):
    if not os.path.exists(audio_file):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ {audio_file} ä¸å­˜åœ¨")
        return

    print(f"ğŸ™ï¸ æ­£åœ¨å¤„ç†éŸ³é¢‘: {os.path.basename(audio_file)}...")
    start_time = time.time()

    # æ‰§è¡Œæ¨ç† Pipeline
    # generate() å†…éƒ¨æµç¨‹:
    # 1. VAD æ‰«æéŸ³é¢‘ï¼Œç”Ÿæˆæ—¶é—´æˆ³åˆ—è¡¨
    # 2. æ ¹æ®æ—¶é—´æˆ³åˆ‡åˆ†éŸ³é¢‘ (Chunking)
    # 3. å¯¹æ¯ä¸ª Chunk è¿›è¡Œ SenseVoice æ¨ç† (Inference)
    # 4. åˆå¹¶ç»“æœ (Merging)
    res = model.generate(
        input=audio_file,
        cache={},
        language="auto",  # è‡ªåŠ¨æ£€æµ‹è¯­è¨€ (zh, en, yue, ja, ko)
        use_itn=True,     # å¼€å¯é€†æ–‡æœ¬æ ‡å‡†åŒ– (ä¾‹å¦‚: "ä¸€ç™¾" -> "100")
        batch_size_s=60,  # åŠ¨æ€æ‰¹å¤„ç†ï¼šæ¯æ‰¹å¤„ç† 60ç§’ çš„éŸ³é¢‘æ•°æ®
        merge_vad=True,   # å°†åˆ‡ç¢çš„ VAD ç‰‡æ®µåˆå¹¶æˆæ•´å¥
    )
    
    inference_time = time.time() - start_time
    
    # ç»“æœè§£æ
    # è¾“å‡ºå¯èƒ½åŒ…å«è¯´è¯äººä¿¡æ¯ï¼Œéœ€è¦åˆ†åˆ«å¤„ç†
    if res:
        # res å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸ï¼Œéœ€è¦åˆ†åˆ«å¤„ç†
        if isinstance(res, list):
            if len(res) > 0:
                result_item = res[0]
                if isinstance(result_item, dict):
                    text = rich_transcription_postprocess(result_item.get("text", ""))
                    # ç§»é™¤ emoji
                    text = remove_emoji(text)
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯´è¯äººä¿¡æ¯
                    speaker_info = result_item.get("spk", None)
                    if speaker_info:
                        print(f"ğŸ“¢ æ£€æµ‹åˆ°è¯´è¯äººä¿¡æ¯")
                        # æ ¼å¼åŒ–è¯´è¯äººä¿¡æ¯ï¼ˆä¸æ˜¾ç¤º timestampï¼‰
                        speaker_texts = []
                        if isinstance(speaker_info, list):
                            for spk in speaker_info:
                                if isinstance(spk, dict):
                                    spk_id = spk.get("spk_id", "Unknown")
                                    spk_text = spk.get("text", "") or spk.get("sentence", "")
                                    if spk_text:
                                        speaker_texts.append(f"è¯´è¯äºº {spk_id}: {spk_text}")
                        if speaker_texts:
                            print("\n".join(speaker_texts))
                    print(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f}s")
                    return text
                else:
                    text = rich_transcription_postprocess(result_item if result_item else "")
                    text = remove_emoji(text)
                    print(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f}s")
                    return text
        elif isinstance(res, dict):
            text = rich_transcription_postprocess(res.get("text", ""))
            # ç§»é™¤ emoji
            text = remove_emoji(text)
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯´è¯äººä¿¡æ¯
            speaker_info = res.get("spk", None)
            if speaker_info:
                print(f"ğŸ“¢ æ£€æµ‹åˆ°è¯´è¯äººä¿¡æ¯")
                # æ ¼å¼åŒ–è¯´è¯äººä¿¡æ¯ï¼ˆä¸æ˜¾ç¤º timestampï¼‰
                speaker_texts = []
                if isinstance(speaker_info, list):
                    for spk in speaker_info:
                        if isinstance(spk, dict):
                            spk_id = spk.get("spk_id", "Unknown")
                            spk_text = spk.get("text", "") or spk.get("sentence", "")
                            if spk_text:
                                speaker_texts.append(f"è¯´è¯äºº {spk_id}: {spk_text}")
                if speaker_texts:
                    print("\n".join(speaker_texts))
            print(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f}s")
            return text
        else:
            text = str(res)
            text = remove_emoji(text)
            print(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f}s")
            return text
    else:
        return "æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³"

if __name__ == "__main__":
    # æ ¹æ®å¤„ç†æ¨¡å¼é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
    if PROCESSING_MODE == "cascaded":
        # çº§è”æ¨¡å¼ï¼šå…ˆ Paraformer åš diarizationï¼Œå†ç”¨ SenseVoice è¯†åˆ«
        print("="*60)
        print("ğŸš€ ä½¿ç”¨çº§è”æ¨¡å¼å¤„ç†")
        print("="*60)
        
        # å¯¼å…¥çº§è”ç³»ç»Ÿæ¨¡å—
        try:
            from run_cascaded_system import (
                setup_cascaded_models,
                process_audio_cascaded,
                format_cascaded_result
            )
            
            # åŠ è½½æ¨¡å‹
            paraformer_model, sensevoice_model = setup_cascaded_models()
            
            # å¤„ç† recordings ç›®å½•ä¸‹çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
            recordings_dir = "/Users/zhengyidi/AutoVoice/recordings"
            
            if not os.path.exists(recordings_dir):
                print(f"âŒ é”™è¯¯: ç›®å½• {recordings_dir} ä¸å­˜åœ¨")
            else:
                # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
                audio_files = [f for f in os.listdir(recordings_dir) 
                              if f.endswith(('.webm', '.mp3', '.wav', '.m4a', '.flac'))]
                
                if not audio_files:
                    print(f"âš ï¸ åœ¨ {recordings_dir} ä¸­æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
                else:
                    print(f"\nğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\n")
                    
                    for idx, audio_file in enumerate(sorted(audio_files), 1):
                        audio_path = os.path.join(recordings_dir, audio_file)
                        print(f"\n{'='*60}")
                        print(f"æ–‡ä»¶ {idx}/{len(audio_files)}: {audio_file}")
                        print(f"{'='*60}")
                        
                        try:
                            # çº§è”å¤„ç†
                            final_results = process_audio_cascaded(
                                audio_path, paraformer_model, sensevoice_model
                            )
                            
                            # æ ¼å¼åŒ–è¾“å‡º
                            formatted_result = format_cascaded_result(final_results, audio_file)
                            
                            print("\n" + formatted_result)
                            
                            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                            output_file = os.path.join(
                                recordings_dir, 
                                f"{os.path.splitext(audio_file)[0]}_cascaded_transcription.txt"
                            )
                            with open(output_file, 'w', encoding='utf-8') as f:
                                f.write(formatted_result)
                            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}\n")
                            
                        except Exception as e:
                            print(f"âŒ å¤„ç†æ–‡ä»¶ {audio_file} æ—¶å‡ºé”™: {str(e)}")
                            import traceback
                            traceback.print_exc()
                    
                    print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
        
        except ImportError as e:
            print(f"âŒ å¯¼å…¥çº§è”ç³»ç»Ÿæ¨¡å—å¤±è´¥: {str(e)}")
            print("è¯·ç¡®ä¿ run_cascaded_system.py æ–‡ä»¶å­˜åœ¨")
            import traceback
            traceback.print_exc()
    
    else:
        # ç›´æ¥æ¨¡å¼ï¼šä½¿ç”¨å•ä¸€æ¨¡å‹
        print("="*60)
        print("ğŸš€ ä½¿ç”¨ç›´æ¥æ¨¡å¼å¤„ç†")
        print("="*60)
        
        # 1. åŠ è½½æ¨¡å‹
        model = setup_model()
        
        # 2. å¤„ç† recordings ç›®å½•ä¸‹çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        recordings_dir = "/Users/zhengyidi/AutoVoice/recordings"
        
        if not os.path.exists(recordings_dir):
            print(f"âŒ é”™è¯¯: ç›®å½• {recordings_dir} ä¸å­˜åœ¨")
        else:
            # è·å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
            audio_files = [f for f in os.listdir(recordings_dir) 
                          if f.endswith(('.webm', '.mp3', '.wav', '.m4a', '.flac'))]
            
            if not audio_files:
                print(f"âš ï¸ åœ¨ {recordings_dir} ä¸­æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
            else:
                print(f"\nğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...\n")
                
                for idx, audio_file in enumerate(sorted(audio_files), 1):
                    audio_path = os.path.join(recordings_dir, audio_file)
                    print(f"\n{'='*60}")
                    print(f"æ–‡ä»¶ {idx}/{len(audio_files)}: {audio_file}")
                    print(f"{'='*60}")
                    
                    result = process_audio(model, audio_path)
                    
                    if result:
                        print("\n" + "="*20 + " è¯†åˆ«ç»“æœ " + "="*20)
                        print(result)
                        print("="*50)
                        
                        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
                        output_file = os.path.join(recordings_dir, f"{os.path.splitext(audio_file)[0]}_transcription.txt")
                        with open(output_file, 'w', encoding='utf-8') as f:
                            f.write(f"éŸ³é¢‘æ–‡ä»¶: {audio_file}\n")
                            f.write(f"è¯†åˆ«ç»“æœ:\n{result}\n")
                        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}\n")
                
                print(f"\nâœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
        
        # ç»“æœè§£é‡Šï¼š
        # è¾“å‡ºå¯èƒ½åŒ…å«ç±»ä¼¼ <|zh|><|NEUTRAL|><|Speech|> çš„æ ‡ç­¾
        # <|zh|>: è¯­è¨€
        # <|NEUTRAL|>: æƒ…æ„Ÿ (HAPPY, SAD, ANGRY, NEUTRAL)
        # <|Speech|>: äº‹ä»¶ (å¯èƒ½åŒ…å« BGM, Laughter ç­‰)